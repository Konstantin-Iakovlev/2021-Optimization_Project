/%% ============================================
%% ================ Preambule =================
%% ============================================
\documentclass[]{scrartcl}
\usepackage[margin = 0.5in]{geometry}

\usepackage[pdftex,unicode, 
colorlinks=true,
linkcolor = blue]{hyperref}	% нумерование страниц, ссылки!!!!ИМЕННО В ТАКОМ ПОРЯДКЕ СО СЛЕДУЮЩИМ ПАКЕТОМ
%\usepackage[warn]{mathtext}				% Поддержка русского текста в формулах
\usepackage[T1, T2A]{fontenc}			% Пакет выбора кодировки и шрифтов
\usepackage[utf8]{inputenc} 			% любая желаемая кодировка
\usepackage[english]{babel}		% поддержка русского языка
\usepackage{wrapfig}					% Плавающие картинки
\usepackage{amssymb, amsmath}			% стилевой пакет для формул
\usepackage{algorithm}
\usepackage{algorithmic} 
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\ifpdf
\usepackage{cmap} 				% чтобы работал поиск по PDF
\usepackage[pdftex]{graphicx}
%\usepackage{pgfplotstable}		% Для вставки таблиц.
\pdfcompresslevel=9 			% сжимать PDF
\else
\usepackage{graphicx}
\fi

\graphicspath{{./figures/}}
\usepackage{subcaption}
%% ============================================
%% ================ Info =================
%% ============================================
\title{Stochastic Quasi Newton methods for Neural Networks}
\author{\begin{tabular}{c c}
	  	 Yakovlev Konstantin & Daniil Merkulov \\
		 \texttt{iakovlev.kd@phystech.edu} & \texttt{daniil.merkulov@skoltech.ru} 
		\end{tabular}}
\date{Project Proposal}

\begin{document}

\maketitle

\begin{abstract}
В проекте рассматривается стохастический квазиньютоновский метод. Данный проект существенно опирается на работу \cite{journals/siamjo/ByrdHNS16}. Приведенный здесь алгоритм использует классическую формулу обновления BFGS в ее ограниченной форме. Также он является эффективным, робастным, масштабируемым и имеет обещающие перспективы в машинном обучении. В данной работе предлагается реализовать приведенный алгоритм и сравнить его с алгоритмом SGD на примере обучения нейронной сети.


\end{abstract}

\section{Идея}

Прямое применение классических квазиньютоновских методов обновления для детерминированной оптимизации приводит к шумным текущим оценкам, которые оказывают негативное влияние на устойчивость итерации. Идея заключается в том, чтобы использовать  стохастический квазиньютоновский метод, который сможет преодолеть данную проблему.

\subsection{Problem}

Пусть задана некоторая функция $f(w, \xi)$, где $w\in \mathbb{R}^n, ~\xi$ -- случайный вектор $(x, z)$. Можно смотреть на пару $(x, z)$ как на объект и ответ соответственно. Обычно в машинном обучении функция $f(w, \xi)$ имеет вид:

\begin{equation}
f(w, \xi) = \ell(h(w; x_i); z_i),
\end{equation}
где $\ell$ -- неотрицательная функция потерь, $h$ -- модель, параметризованная вектором $w$, $\{x_i, z_i\}_{i=1}^N$ -- обучающая выборка. Определим эмпирический риск как:

\begin{equation}
F(w) = \frac{1}{N}\sum_{i=1}^Nf(w; x_i, z_i)
\end{equation}

Определим множество $\mathcal{S} \subset \{1, \ldots, N\}$. Пусть $b = |\mathcal{S}| \ll N$. Тогда можно записать оценку для эмпирического риска:

\begin{equation}
\label{risk_approx}
\widehat{\nabla}F(w) = \frac{1}{b}\sum_{i\in \mathcal{S}}\nabla f(w; x_i, z_i)
\end{equation}

Сформулируем задачу оптимизации следующим образом:

\begin{equation}
\min_{w\in \mathbb{R}^n}F(w)
\end{equation}


TODO:...Дальнейшее описание:

%https://github.com/cthl/sqgn

\begin{equation}
\label{hess_risk_approx}
\widehat{\nabla}^2F(w) = \frac{1}{b_H}\sum_{i\in\mathcal{S}_H}\nabla^2f(w; x_i, z_i)
\end{equation}



Запишем итоговый алгоритм:

\begin{algorithm}[h!]
	\caption{Stochastic Quasi-Newton Method (SQN)}
	\label{SQN}
	\begin{algorithmic}[1]
		\REQUIRE начальные параметры $w^1$; натуральные числа $M, L$; последовательность шагов $\{\alpha_k\}$
		\STATE Инициализируем $t = -1$
		\STATE Инициализируем $\overline{w}_t = 0$		
		\FOR {$k=1,2,...,$}
		\STATE Выберем множество $\mathcal{S} \subset\{1, \ldots, N\}$
		\STATE Вычислим $\widehat{\nabla}F(w^k)$ по формуле \ref{risk_approx}
		\STATE $\overline{w}_t = \overline{w}_t + w^k$
		\IF {$k\leq 2L$}
			\STATE $w^{k+1} = w^k - \alpha_k \widehat{\nabla}F(w^k)$
		\ELSE 
			\STATE $w^{k+1} = w^k - \alpha_kH_t\widehat{\nabla}F(w^k)$, $H_t$ оперделяется алгоритмом \ref{hess_updating}
		\ENDIF
		
		\IF {$k ~\%~ L = 0$}
			\STATE $t = t+1$
			\STATE $\overline{w}_t = \overline{w}_t / L$
			\IF {$t > 0$}
				\STATE Выбираем $\mathcal{S}_H \subset \{1,\ldots,N\}$, чтобы определить $\widehat{\nabla}^2F(\overline{w}_t)$ по формуле \ref{hess_risk_approx}
				 \STATE Вычислим $s_t = \overline{w}_t - \overline{w}_{t-1}, ~y_t = \widehat{\nabla}^2F(\overline{w}_t)(\overline{w}_t - \overline{w}_{t-1})$ \label{st_yt}
			\ENDIF
			\STATE $\overline{w}_t = 0$
		\ENDIF
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

Запишем алгоритм обновления гессиана:

\begin{algorithm}[h!]
	\caption{Обновление Гессиана}
	\label{hess_updating}
	\begin{algorithmic}[1]
	\REQUIRE Счетчик обновлений $t$, натуральное число $M$ , коррекционные пары $(s_j, y_j)$,\\$j = t - \min(t, M) + 1, \ldots, t$
	\ENSURE $H_t$
	\STATE Инициализировать $H = (s_t^\top y_t)/(y_t^\top y_t)I$, где $s_t, y_t$ вычислены на шаге \ref{st_yt} алгоритма \ref{SQN}
	\FOR {$j = t - \min(t, M) + 1, \ldots, t$}
		\STATE $\rho_j = 1/y_j^\top s_j$
		\STATE Применяем  BFGS формулу $H_t = (I - \rho_js_jy_j^\top)H(I - \rho_jy_js_j^\top) + \rho_js_js_j^\top$
	\ENDFOR
	\RETURN $H_t$
	\end{algorithmic}
\end{algorithm}



\section{Outcomes}
%Опишите, что конкретно будет выходом Вашего проекта (код, теорема, численные эксперименты, телеграм бот, веб сайт, приложение, рассказ)
Результатами данного проекта являются:
\begin{itemize}
\item Реализация алгоритма SQN, описанного в работе \cite{journals/siamjo/ByrdHNS16}, с использованием фреймворка jax \cite{jax2018github}.

\item Сравнение алгоитма SQN \cite{journals/siamjo/ByrdHNS16} с алгоритмом SGD на примере обучения нейронной сети. Метрики качества приведены далее.
\end{itemize}

%\begin{figure}[h!]
%	\center{\includegraphics[width=0.3\linewidth]{figures/test.pdf}}
%	\caption{Картинка, иллюстрирующая идею / выход проекта}
%\end{figure}


\section{Литературный обзор}

В последнее время в машинном обучении растет интерес к очень большим моделям. В большинстве крупномасштабных задач обучения таких моделей используются алгоритмы стохастической оптимизации, которые обновляют параметры модели на основе небольшого количества обучающих данных. Приведем обзор некоторых работ по стохастической оптимизации.

В работе \cite{journals/siamjo/ByrdHNS16} рассматривалась задача минимизации выпуклой стохастической функции. Предложен квазиньютоновский метод, испольщующий формулу обновления BFGS с ограниченной памятью. Здесь приведен метод получения стабильных оценок гессиана. Идея состоит в том, чтобы вычислить средние оценки кривизны через регулярные промежутки времени с помощью некоторой подвыборки.

TODO: adaQN https://arxiv.org/pdf/1511.01169.pdf.


Также рассматривалась задача минимизации невыпуклой стохастической функции \cite{journals/siamjo/WangMGL17}. В данной работе представлена общая структура стохастических квази-ньютоновских методов невыпуклой стохастической оптимизации. Также был предложен алгоритм SdLBFGS (stochastic damped L-BFGS), соответствующий данной структуре. Были приведены и доказаны достаточные условия сходимости алгоритма. Кроме того, существует улучшение данного алгоритма -- Sd-REG-LBGS \cite{journals/corr/abs-1912-04456}. Эксперименты показывают, что новая версия алгоритма в целом превосходит SdLBFGS. Также для этого метода получены достаточные условия сходимости.

В некоторых работах рассматривалась задача минимизации сильно выпуклой функции на римановских многообразиях \cite{journals/corr/RoychowdhuryP17}. Данный подход является нестандартным, поскольку подобные алгоритмы хорошо изучены лишь в случае евклидовых пространств. Для решения данной задачи в работе был предложен алгоритм Riemannian Stochastic VR L-BFGS. Вычислительные эксперименты показали, что построенный алгоритм превосходит другие алгоритмы римановой оптимизации, а также работает значительно лучше, чем один из самых быстрых стохастических алгоритмов в евклидовом пространстве, VR-PCA \cite{journals/corr/Shamir14b}. Алгоритм VR-PCA использует технику уменьшения стохастической градиентной дисперсии \cite{oai:repository.ust.hk:1783.1-98257}.  Стоит также  отметить, что алгоритм имеет линейную сходимость к оптимальному решению. 

Рассматривались также подходы с использованием метода ускоренного градиента Нестерова \cite{journals/corr/abs-1909-03621}. В данной работе рассматривалась крупномасштабная задача минимизации невыпуклой стохастической функции. К квазиньютоновскому стохастическому методу были добавлены некоторые модификации для ускорения сходимости. Вычислительные эксперименты показывают, что предлагаемые метод oNAQ (online NAQ)  превосходит традиционные методы oBFGS (online BFGS) \cite{journals/jmlr/SchraudolphYG07} с одинаковыми затратами на вычисление и память.

Также был предложен новый класс стохастических адаптивных методов минимизации самосогласованных стохастических функций \cite{conf/icml/ZhouGG17}. Ключевой идеей адаптивных методов является то, что шаг $t_k$ на каждой итерации может быть вычислен аналитически, используя только локальную информацию. В частности, методы этого класса включают в себя модификации градиентного спуска (GD) и BFGS. Также в работе \cite{journals/oms/GaoG19} было показано, что метод BFGS с адаптивным шагом имеет сверхлинейную сходимость для сильно выпуклых самосогласованных функций. 


\section{Метрики качества}

Метрики для второй части моего проекта -- сравнение SGD и SQN:

\begin{itemize}
\item Значение оптимизируемой функции после одинакового времени работы на CPU/GPU.
\item Значение оптимизируемой функции после одинакового количества итераций.
\item Количество вызовов оптимизируемой функции и ее градиента после одинакового количества итераций.
\end{itemize}

\section{Примерный план}

\begin{enumerate}
\item Первым делом реализуем SQN с использованием фреймворка JAX \cite{jax2018github}. По сути, в нашем проекте мы представили псевдокод. Срок: 21 марта.
\item Затем займемся планированием эксперимента для сравнения SGD и SQN. В качестве архитектуры нейронной сети можно взять LeNet, а в качестве выборки -- MNIST. Нейронная сеть будет реализована на PyTorch. В качестве оптимизатора будет подаваться SGD и SQN. Срок: 31 марта. 
\end{enumerate}

\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}
